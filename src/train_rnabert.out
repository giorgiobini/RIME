nohup: ignoring input
03/21/2023 18:34:30 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 2, distributed training: False, 16-bits training: False
03/21/2023 18:34:30 - INFO - transformers.configuration_utils -   loading configuration file ../dataset/pre_trained_DNABERT/6-new-12w-0/config.json
03/21/2023 18:34:30 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

03/21/2023 18:34:30 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt from cache at /home/gbini/.cache/torch/transformers/ea1474aad40c1c8ed4e1cb7c11345ddda6df27a857fb29e1d4c901d9b900d32d.26f8bd5a32e49c2a8271a46950754a4a767726709b7741c68723bc1db840a87e
03/21/2023 18:34:30 - INFO - transformers.modeling_utils -   loading weights file ../dataset/pre_trained_DNABERT/6-new-12w-0/pytorch_model.bin
03/21/2023 18:34:33 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, beta1=0.9, beta2=0.98, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda', index=0), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='../dataset/pre_trained_DNABERT/rna_data/fake_6mers.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=0.0004, line_by_line=True, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.025, model_name_or_path='../dataset/pre_trained_DNABERT/6-new-12w-0', model_type='dna', n_gpu=2, n_process=24, no_cuda=False, num_train_epochs=1.0, output_dir='../dataset/pre_trained_DNABERT/output6', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=8, save_steps=1000, save_total_limit=30, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='dna6', train_data_file='../dataset/pre_trained_DNABERT/rna_data/test_6mers.txt', warmup_steps=10000, weight_decay=0.01)
03/21/2023 18:34:33 - INFO - __main__ -   Loading features from cached file ../dataset/pre_trained_DNABERT/rna_data/dna_cached_lm_512_test_6mers.txt
03/21/2023 18:34:34 - INFO - __main__ -   ***** Running training *****
03/21/2023 18:34:34 - INFO - __main__ -     Num examples = 35729
03/21/2023 18:34:34 - INFO - __main__ -     Num Epochs = 1
03/21/2023 18:34:34 - INFO - __main__ -     Instantaneous batch size per GPU = 8
03/21/2023 18:34:34 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16
03/21/2023 18:34:34 - INFO - __main__ -     Gradient Accumulation steps = 1
03/21/2023 18:34:34 - INFO - __main__ -     Total optimization steps = 2234
03/21/2023 18:34:34 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step
03/21/2023 18:34:34 - INFO - __main__ -     Continuing training from epoch 0
03/21/2023 18:34:34 - INFO - __main__ -     Continuing training from global step 0
03/21/2023 18:34:34 - INFO - __main__ -     Will skip the first 0 steps in the first epoch
DEVICE:cuda:0
============================================================
<class 'transformers.tokenization_dna.DNATokenizer'>
Model loaded from checkpoint
0
2234
1
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/2234 [00:00<?, ?it/s][A/home/gbini/miniconda3/envs/rnabert/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
/home/gbini/miniconda3/envs/rnabert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
03/21/2023 18:45:06 - INFO - transformers.configuration_utils -   Configuration saved in ../dataset/pre_trained_DNABERT/output6/checkpoint-1000/config.json
03/21/2023 18:45:07 - INFO - transformers.modeling_utils -   Model weights saved in ../dataset/pre_trained_DNABERT/output6/checkpoint-1000/pytorch_model.bin
03/21/2023 18:45:07 - INFO - __main__ -   Saving model checkpoint to ../dataset/pre_trained_DNABERT/output6/checkpoint-1000
03/21/2023 18:45:07 - INFO - __main__ -   Saving optimizer and scheduler states to ../dataset/pre_trained_DNABERT/output6/checkpoint-1000

Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1420/2234 [15:00<08:36,  1.58it/s][A
Iteration:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1420/2234 [15:20<08:36,  1.58it/s][A03/21/2023 18:55:42 - INFO - transformers.configuration_utils -   Configuration saved in ../dataset/pre_trained_DNABERT/output6/checkpoint-2000/config.json
03/21/2023 18:55:42 - INFO - transformers.modeling_utils -   Model weights saved in ../dataset/pre_trained_DNABERT/output6/checkpoint-2000/pytorch_model.bin
03/21/2023 18:55:42 - INFO - __main__ -   Saving model checkpoint to ../dataset/pre_trained_DNABERT/output6/checkpoint-2000
03/21/2023 18:55:42 - INFO - __main__ -   Saving optimizer and scheduler states to ../dataset/pre_trained_DNABERT/output6/checkpoint-2000
Iteration: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2234/2234 [23:34<00:00,  1.58it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [23:34<00:00, 1414.92s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [23:34<00:00, 1414.92s/it]
03/21/2023 18:58:09 - INFO - __main__ -    global_step = 2234, average loss = 0.9918605994729988
03/21/2023 18:58:09 - INFO - __main__ -   Saving model checkpoint to ../dataset/pre_trained_DNABERT/output6
03/21/2023 18:58:09 - INFO - transformers.configuration_utils -   Configuration saved in ../dataset/pre_trained_DNABERT/output6/config.json
03/21/2023 18:58:09 - INFO - transformers.modeling_utils -   Model weights saved in ../dataset/pre_trained_DNABERT/output6/pytorch_model.bin
03/21/2023 18:58:09 - INFO - transformers.configuration_utils -   loading configuration file ../dataset/pre_trained_DNABERT/output6/config.json
03/21/2023 18:58:09 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

03/21/2023 18:58:09 - INFO - transformers.modeling_utils -   loading weights file ../dataset/pre_trained_DNABERT/output6/pytorch_model.bin
03/21/2023 18:58:11 - INFO - transformers.tokenization_utils -   Model name '../dataset/pre_trained_DNABERT/output6' not found in model shortcut name list (dna3, dna4, dna5, dna6). Assuming '../dataset/pre_trained_DNABERT/output6' is a path, a model identifier, or url to a directory containing tokenizer files.
03/21/2023 18:58:11 - INFO - transformers.tokenization_utils -   Didn't find file ../dataset/pre_trained_DNABERT/output6/added_tokens.json. We won't load it.
03/21/2023 18:58:11 - INFO - transformers.tokenization_utils -   loading file ../dataset/pre_trained_DNABERT/output6/vocab.txt
03/21/2023 18:58:11 - INFO - transformers.tokenization_utils -   loading file None
03/21/2023 18:58:11 - INFO - transformers.tokenization_utils -   loading file ../dataset/pre_trained_DNABERT/output6/special_tokens_map.json
03/21/2023 18:58:11 - INFO - transformers.tokenization_utils -   loading file ../dataset/pre_trained_DNABERT/output6/tokenizer_config.json
03/21/2023 18:58:11 - INFO - __main__ -   Evaluate the following checkpoints: ['../dataset/pre_trained_DNABERT/output6']
03/21/2023 18:58:11 - INFO - transformers.configuration_utils -   loading configuration file ../dataset/pre_trained_DNABERT/output6/config.json
03/21/2023 18:58:11 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "num_rnn_layer": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "rnn": "lstm",
  "rnn_dropout": 0.0,
  "rnn_hidden": 768,
  "split": 10,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 4101
}

03/21/2023 18:58:11 - INFO - transformers.modeling_utils -   loading weights file ../dataset/pre_trained_DNABERT/output6/pytorch_model.bin
03/21/2023 18:58:13 - INFO - __main__ -   Loading features from cached file ../dataset/pre_trained_DNABERT/rna_data/dna_cached_lm_512_fake_6mers.txt
03/21/2023 18:58:13 - INFO - __main__ -   ***** Running evaluation  *****
03/21/2023 18:58:13 - INFO - __main__ -     Num examples = 500
03/21/2023 18:58:13 - INFO - __main__ -     Batch size = 16
============================================================
<class 'transformers.tokenization_dna.DNATokenizer'>
Evaluating:   0%|          | 0/32 [00:00<?, ?it/s]Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:07<00:00,  4.26it/s]
03/21/2023 18:58:20 - INFO - __main__ -   ***** Eval results  *****
03/21/2023 18:58:20 - INFO - __main__ -     perplexity = tensor(2.5500)

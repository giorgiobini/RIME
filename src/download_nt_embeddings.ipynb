{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import random_split, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from dataset.data import (\n",
    "    RNADataset,\n",
    "    ROOT_DIR,\n",
    "    EasyPosAugment,\n",
    "    RegionSpecNegAugment,\n",
    "    InteractionSelectionPolicy,\n",
    "    EasyNegAugment,\n",
    "    HardPosAugment,\n",
    "    HardNegAugment,\n",
    "    plot_sample,\n",
    "    #plot_sample2,\n",
    "    seed_everything,\n",
    "    MAX_RNA_SIZE,\n",
    ")\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import nucleotide_transformer\n",
    "from nucleotide_transformer.mypretrained import get_pretrained_model\n",
    "import time\n",
    "random_key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pos_width_multipliers = {20:0.5, MAX_RNA_SIZE: 0.5}\n",
    "pos_height_multipliers = pos_width_multipliers\n",
    "neg_width_windows = {\n",
    "    (50, 1000): 0.5,\n",
    "    (1000, MAX_RNA_SIZE): 0.5,\n",
    "}\n",
    "neg_height_windows = neg_width_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_rna_files_dir = os.path.join(ROOT_DIR, \"dataset\", \"rna_rna_pairs\")\n",
    "processed_files_dir = os.path.join(ROOT_DIR, \"dataset\", \"processed_files\")\n",
    "nt_data_dir = os.path.join(processed_files_dir, \"nt_data\")\n",
    "embedding_dir = os.path.join(nt_data_dir, \"embeddings\")\n",
    "metadata_dir = os.path.join(nt_data_dir, \"metadata\")\n",
    "nt_dir =  os.path.join(ROOT_DIR, \"NT_dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data = 'val' \n",
    "assert set_data in ['training', 'val', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566bff65000940af96e1a11a98593b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta = {}\n",
    "id_couple = 0\n",
    "_SUBSET_SIZE: int = 1\n",
    "seed_everything(seed)\n",
    "for policy in (\n",
    "    EasyPosAugment(\n",
    "        per_sample=1,\n",
    "        interaction_selection=InteractionSelectionPolicy.LARGEST,\n",
    "        width_multipliers=pos_width_multipliers,\n",
    "        height_multipliers=pos_height_multipliers,\n",
    "    ),\n",
    "    # EasyNegAugment(\n",
    "    #     per_sample=1,\n",
    "    #     width_windows=neg_width_windows,\n",
    "    #     height_windows=neg_height_windows,\n",
    "    # ),\n",
    "    # HardPosAugment(\n",
    "    #     per_sample=1,\n",
    "    #     interaction_selection=InteractionSelectionPolicy.RANDOM_ONE,\n",
    "    #     min_width_overlap=0.3,\n",
    "    #     min_height_overlap=0.3,\n",
    "    #     width_multipliers=pos_width_multipliers,\n",
    "    #     height_multipliers=pos_height_multipliers,\n",
    "    # ),\n",
    "    # HardNegAugment(\n",
    "    #     per_sample=1,\n",
    "    #     width_windows=neg_width_windows,\n",
    "    #     height_windows=neg_height_windows,\n",
    "    # ),\n",
    "    # RegionSpecNegAugment(\n",
    "    #     per_sample=1,\n",
    "    #     width_windows=pos_width_multipliers,\n",
    "    #     height_windows=neg_height_windows,\n",
    "    # ),\n",
    "):\n",
    "    start_time = time.time()\n",
    "\n",
    "    dataset = RNADataset(\n",
    "        gene_info_path=os.path.join(processed_files_dir, \"df_genes.csv\"),\n",
    "        interactions_path=os.path.join(\n",
    "            processed_files_dir, \"final_df.csv\"\n",
    "        ),\n",
    "        subset_file=os.path.join(\n",
    "            rna_rna_files_dir, \"gene_pairs_val.txt\"\n",
    "        ),\n",
    "        augment_policies=[\n",
    "            policy,\n",
    "        ],\n",
    "    )\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        d = {\n",
    "            'id_sample':id_couple,\n",
    "            'couple':sample.couple_id,\n",
    "            'gene1':sample.gene1,\n",
    "            'gene2':sample.gene2,\n",
    "            'x1':sample.bbox.x1,\n",
    "            'x2':sample.bbox.x2,\n",
    "            'y1':sample.bbox.y1,\n",
    "            'y2':sample.bbox.y2,\n",
    "            'interacting':sample.interacting,\n",
    "            'policty':sample.policy,\n",
    "            'cdna1':sample.gene1_info[\"cdna\"][sample.bbox.x1:sample.bbox.x2],\n",
    "            'cdna2':sample.gene2_info[\"cdna\"][sample.bbox.y1:sample.bbox.y2],\n",
    "            'seed_x1':sample.seed_interaction_bbox.x1,\n",
    "            'seed_x2':sample.seed_interaction_bbox.x2,\n",
    "            'seed_y1':sample.seed_interaction_bbox.y1,\n",
    "            'seed_y2':sample.seed_interaction_bbox.y2,\n",
    "            'protein_coding1':sample.gene1_info[\"protein_coding\"],\n",
    "            'protein_coding2':sample.gene2_info[\"protein_coding\"],\n",
    "            'original_length1':len(sample.gene1_info[\"cdna\"]),\n",
    "            'original_length2':len(sample.gene2_info[\"cdna\"]),\n",
    "        }\n",
    "        meta[id_couple] = d\n",
    "        id_couple += 1\n",
    "#         if sample.interacting:\n",
    "#             sequences_pos.append()\n",
    "#         else:\n",
    "#             sequences_pos.append()\n",
    "            \n",
    "        if i == 1000:\n",
    "            break\n",
    "        #print(sample.gene1, sample.gene2)\n",
    "        continue\n",
    "meta = pd.DataFrame.from_dict(meta, 'index')\n",
    "meta.to_csv(os.path.join(metadata_dir, f'{set_data}.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_embeddings(outs, tokens, layer):\n",
    "    # Retrieve embeddings\n",
    "    embeddings = outs[f\"embeddings_{layer}\"][:, 1:, :]  # removing CLS token\n",
    "    padding_mask = jnp.expand_dims(tokens[:, 1:] != tokenizer.pad_token_id, axis=-1)\n",
    "    masked_embeddings = embeddings * padding_mask  # multiply by 0 pad tokens embeddings\n",
    "    sequences_lengths = jnp.sum(padding_mask, axis=1)\n",
    "    mean_embeddings = jnp.sum(masked_embeddings, axis=1) / sequences_lengths\n",
    "    return mean_embeddings\n",
    "\n",
    "def infer(sequences, forward_fn, tokenizer, parameters, random_key):\n",
    "    tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "    tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "    tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "    # Infer\n",
    "    outs = forward_fn.apply(parameters, random_key, tokens)\n",
    "    return outs, tokens\n",
    "\n",
    "def save_data_to_folder(data, labels, ids, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    for i in range(len(data)):\n",
    "        sample = data[i]\n",
    "        label = labels[i]\n",
    "        id_sample = ids[i]\n",
    "        if label == 0:\n",
    "            class_folder = os.path.join(folder_path, 'class_0')\n",
    "        else:\n",
    "            class_folder = os.path.join(folder_path, 'class_1')\n",
    "        if not os.path.exists(class_folder):\n",
    "            os.makedirs(class_folder)\n",
    "        np.save(os.path.join(class_folder, f'{id_sample}.npy'), sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "n_batch = int(meta.shape[0]/batch_size)\n",
    "slices = np.linspace(0, meta.shape[0], n_batch, dtype = np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_layers_to_save = (20, 24, 28, 32)\n",
    "\n",
    "model_name = '2B5_multi_species'\n",
    "\n",
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    mixed_precision=False,\n",
    "    embeddings_layers_to_save=embeddings_layers_to_save,\n",
    "    attention_maps_to_save=(),\n",
    "    max_positions=1000,\n",
    "    chkpt_dir = os.path.join(nt_dir, 'checkpoints')\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in tqdm(range(len(slices)-1)):\n",
    "    \n",
    "    meta_slice = meta[slices[i]:slices[i+1]]\n",
    "    labels = list(meta[slices[i]:slices[i+1]].interacting.values.astype(int))\n",
    "    ids = list(meta[slices[i]:slices[i+1]].id_sample.values)\n",
    "    \n",
    "    sequences1 = list(meta_slice.cdna1.values)\n",
    "    outs1, tokens1 = infer(sequences1, forward_fn, tokenizer, parameters, random_key)\n",
    "    \n",
    "    sequences2 = list(meta_slice.cdna2.values)\n",
    "    outs2, tokens2 = infer(sequences2, forward_fn, tokenizer, parameters, random_key)\n",
    "    \n",
    "    for layer in embeddings_layers_to_save:\n",
    "        layer_folder = os.path.join(embedding_dir, str(layer))\n",
    "        if not os.path.exists(layer_folder):\n",
    "            os.makedirs(layer_folder)\n",
    "            \n",
    "        mean_embeddings1 = retrieve_embeddings(outs1, tokens1, layer) #shape is (batch_size, 2560)\n",
    "        mean_embeddings2 = retrieve_embeddings(outs2, tokens2, layer) #shape is (batch_size, 2560)\n",
    "        \n",
    "        #concatenate the two embeddings (check if I am doing this properly, with the rigth axis)\n",
    "        embeddings = np.concatenate((mean_embeddings1, mean_embeddings2), axis=1) #shape is (2*batch_size, 5120)\n",
    "        \n",
    "        save_data_to_folder(embeddings, labels, ids,  os.path.join(layer_folder, set_data))\n",
    "print(f\"Total time to process batch: {(time.time()-start_time)/60} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "28f202d77f1a85c5cab767fd2089c1bba19c245ba743de17ef12d30078485197"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import nucleotide_transformer\n",
    "from nucleotide_transformer.mypretrained import get_pretrained_model\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_data = 'training'\n",
    "assert set_data in ['training', 'val', 'test']\n",
    "meta = pd.read_csv(os.path.join(metadata_dir, f'{set_data}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluded = pd.read_csv(os.path.join(metadata_dir, 'excluded_training.txt'), header = None)\n",
    "# done = pd.read_csv(os.path.join(metadata_dir, 'done_training.txt'), header = None)\n",
    "# set1 = meta[(meta.id_sample.isin(excluded[0].values))].cdna1.apply(lambda x: set(x))\n",
    "# set2 = meta[(meta.id_sample.isin(excluded[0].values))].cdna2.apply(lambda x: set(x))\n",
    "# meta[~meta.id_sample.isin(set(done[0]))].to_csv(os.path.join(metadata_dir, 'training_remain.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grouped_mean_embeddings(outs, layer, tokens, tokenizer, k):\n",
    "    # Get the embeddings for the specified layer\n",
    "    embeddings = outs[f\"embeddings_{layer}\"]\n",
    "    \n",
    "    # Remove the CLS token and paddings\n",
    "    embeddings = embeddings[:, 1:, :]\n",
    "    padding_mask = jnp.expand_dims(tokens[:, 1:] != tokenizer.pad_token_id, axis=-1)\n",
    "    masked_embeddings = embeddings * padding_mask\n",
    "    \n",
    "    # Calculate the number of groups\n",
    "    batch_size = masked_embeddings.shape[0]\n",
    "    seq_length = masked_embeddings.shape[1]\n",
    "    num_groups = seq_length // k\n",
    "    \n",
    "    # Reshape the embeddings to form groups\n",
    "    grouped_embeddings = jnp.reshape(masked_embeddings[:, :num_groups*k, :], (batch_size, num_groups, k, -1))\n",
    "    grouped_padding_mask = jnp.reshape(padding_mask[:, :num_groups*k, :], (batch_size, num_groups, k, -1))\n",
    "    \n",
    "    # Calculate the mean embeddings for each group\n",
    "    group_sum_embeddings = jnp.sum(grouped_embeddings, axis=2)\n",
    "    sequence_count_in_groups = grouped_padding_mask.sum(axis=2)\n",
    "    sequence_count_in_groups = jnp.where(sequence_count_in_groups == 0, 1, sequence_count_in_groups)  # to avoid division by zero\n",
    "    group_mean_embeddings = group_sum_embeddings / sequence_count_in_groups\n",
    "    \n",
    "    if k == 999:\n",
    "        group_mean_embeddings = group_mean_embeddings.squeeze()\n",
    "    \n",
    "    return group_mean_embeddings\n",
    "\n",
    "def infer(sequences, forward_fn, tokenizer, parameters, random_key):\n",
    "    tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "    tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "    tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "    # Infer\n",
    "    outs = forward_fn.apply(parameters, random_key, tokens)\n",
    "    return outs, tokens\n",
    "\n",
    "def save_data_to_folder(data, labels, ids, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    for i in range(len(data)):\n",
    "        sample = data[i]\n",
    "        label = labels[i]\n",
    "        id_sample = ids[i]\n",
    "        if label == 0:\n",
    "            class_folder = os.path.join(folder_path, 'class_0')\n",
    "        else:\n",
    "            class_folder = os.path.join(folder_path, 'class_1')\n",
    "        if not os.path.exists(class_folder):\n",
    "            os.makedirs(class_folder)\n",
    "        np.save(os.path.join(class_folder, f'{id_sample}.npy'), sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_layers_to_save = (22, 30)\n",
    "\n",
    "model_name = '2B5_multi_species'\n",
    "\n",
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    mixed_precision=False,\n",
    "    embeddings_layers_to_save=embeddings_layers_to_save,\n",
    "    attention_maps_to_save=(),\n",
    "    max_positions=1000,\n",
    "    chkpt_dir = os.path.join(nt_dir, 'checkpoints')\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 13\n",
    "k = 999\n",
    "n_batch = int(meta.shape[0]/batch_size)\n",
    "slices = np.linspace(0, meta.shape[0], n_batch, dtype = np.int64)\n",
    "\n",
    "k_dir = os.path.join(embedding_dir, str(k))\n",
    "if not os.path.exists(k_dir):\n",
    "    os.makedirs(k_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 14:55:00.453287: W external/xla/xla/service/hlo_rematerialization.cc:2194] Can't reduce memory use below 8.06GiB (8658026496 bytes) by rematerialization; only reduced to 12.00GiB (12888088964 bytes)\n",
      "2023-04-15 14:55:17.713095: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2432] Execution of replica 0 failed: INTERNAL: Failed to allocate 104857600 bytes for new constant\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: Failed to allocate 104857600 bytes for new constant",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(meta[slices[i]:slices[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m.\u001b[39mid_sample\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m      8\u001b[0m sequences1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(meta_slice\u001b[38;5;241m.\u001b[39mcdna1\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m----> 9\u001b[0m outs1, tokens1 \u001b[38;5;241m=\u001b[39m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m sequences2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(meta_slice\u001b[38;5;241m.\u001b[39mcdna2\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     12\u001b[0m outs2, tokens2 \u001b[38;5;241m=\u001b[39m infer(sequences2, forward_fn, tokenizer, parameters, random_key)\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(sequences, forward_fn, tokenizer, parameters, random_key)\u001b[0m\n\u001b[1;32m     33\u001b[0m tokens \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(tokens_ids, dtype\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Infer\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mforward_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs, tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/rnarna/lib/python3.10/site-packages/haiku/_src/transform.py:128\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    122\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    123\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the functions you are transforming use the same names you must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m out, state \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state:\n\u001b[1;32m    130\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf your transformed function uses `hk.\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mget,set}_state` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthen use `hk.transform_with_state`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rnarna/lib/python3.10/site-packages/haiku/_src/transform.py:357\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m base\u001b[38;5;241m.\u001b[39mnew_context(params\u001b[38;5;241m=\u001b[39mparams, state\u001b[38;5;241m=\u001b[39mstate, rng\u001b[38;5;241m=\u001b[39mrng) \u001b[38;5;28;01mas\u001b[39;00m ctx:\n\u001b[1;32m    356\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jax\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mUnexpectedTracerError(unexpected_tracer_hint) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rnarna/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:1916\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1911\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_token_bufs(\n\u001b[1;32m   1912\u001b[0m       results\u001b[38;5;241m.\u001b[39mdisassemble_prefix_into_single_device_arrays(\n\u001b[1;32m   1913\u001b[0m           \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mordered_effects)),\n\u001b[1;32m   1914\u001b[0m       results\u001b[38;5;241m.\u001b[39mconsume_token())\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1916\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxla_executable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1918\u001b[0m   out_arrays \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: Failed to allocate 104857600 bytes for new constant"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for i in range(len(slices)-1):\n",
    "    \n",
    "    meta_slice = meta[slices[i]:slices[i+1]]\n",
    "    labels = list(meta[slices[i]:slices[i+1]].interacting.values.astype(int))\n",
    "    ids = list(meta[slices[i]:slices[i+1]].id_sample.values)\n",
    "    \n",
    "    try:\n",
    "        sequences1 = list(meta_slice.cdna1.values)\n",
    "        outs1, tokens1 = infer(sequences1, forward_fn, tokenizer, parameters, random_key)\n",
    "\n",
    "        sequences2 = list(meta_slice.cdna2.values)\n",
    "        outs2, tokens2 = infer(sequences2, forward_fn, tokenizer, parameters, random_key)\n",
    "\n",
    "        for layer in embeddings_layers_to_save:\n",
    "            layer_folder = os.path.join(k_dir, str(layer))\n",
    "            if not os.path.exists(layer_folder):\n",
    "                os.makedirs(layer_folder)\n",
    "\n",
    "            mean_embeddings1 = calculate_grouped_mean_embeddings(outs1, layer, tokens1, tokenizer, k) #shape is (batch_size, 2560)\n",
    "            mean_embeddings2 = calculate_grouped_mean_embeddings(outs2, layer, tokens2, tokenizer, k) #shape is (batch_size, 2560)\n",
    "\n",
    "            #concatenate the two embeddings (check if I am doing this properly, with the rigth axis)\n",
    "            embeddings = np.concatenate((mean_embeddings1, mean_embeddings2), axis=1) #shape is (2*batch_size, 5120)\n",
    "\n",
    "            #save the embeddings\n",
    "            save_data_to_folder(embeddings, labels, ids, os.path.join(layer_folder, set_data))\n",
    "\n",
    "        del outs1\n",
    "        del outs2\n",
    "    except:\n",
    "        with open(os.path.join(metadata_dir, f\"excluded_{set_data}.txt\"), 'a') as f:\n",
    "            for idx in ids:\n",
    "                f.write(str(idx) + '\\n')\n",
    "\n",
    "    with open(os.path.join(metadata_dir, f\"done_{set_data}.txt\"), 'a') as f:\n",
    "        for idx in ids:\n",
    "            f.write(str(idx) + '\\n')\n",
    "    \n",
    "    if i%190 == 0:\n",
    "        perc = np.round(i/len(slices) * 100, 2)\n",
    "        print(f'{perc}% done in {(time.time()-start_time)/60} minutes')\n",
    "    \n",
    "print(f\"Total time to process batch: {(time.time()-start_time)/60} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "28f202d77f1a85c5cab767fd2089c1bba19c245ba743de17ef12d30078485197"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
